#!/usr/bin/env python3
"""
Optimized Object Detection for Raspberry Pi Real-time Applications
S·ª≠ d·ª•ng YOLO11n v·ªõi NCNN optimization cho hi·ªáu su·∫•t t·ªët nh·∫•t tr√™n RPi
"""

import cv2
import numpy as np
from ultralytics import YOLO
import time
from typing import List, Dict, Tuple, Optional
import yaml
import threading
from queue import Queue

class OptimizedDetector:
    """
    Detector t·ªëi ∆∞u cho Raspberry Pi v·ªõi YOLO11n + NCNN
    """
    
    def __init__(self, config_path: str = "configs/config.yaml", use_ncnn: bool = True):
        """
        Kh·ªüi t·∫°o detector t·ªëi ∆∞u
        
        Args:
            config_path: ƒê∆∞·ªùng d·∫´n file config
            use_ncnn: S·ª≠ d·ª•ng NCNN format (khuy√™n d√πng cho RPi)
        """
        print("üöÄ Kh·ªüi t·∫°o Optimized Detector cho Raspberry Pi...")
        
        # Load config
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # C·∫•u h√¨nh t·ªëi ∆∞u cho RPi
        self.input_size = (416, 416)  # Gi·∫£m t·ª´ 640x640 ƒë·ªÉ tƒÉng t·ªëc
        self.confidence = 0.4  # TƒÉng threshold ƒë·ªÉ gi·∫£m false positives
        self.iou_threshold = 0.5
        self.use_ncnn = use_ncnn
        
        # Load model
        self._load_model()
        
        # Classes quan tr·ªçng cho giao th√¥ng VN
        self.traffic_classes = {
            0: 'person',        # Ng∆∞·ªùi ƒëi b·ªô
            1: 'bicycle',       # Xe ƒë·∫°p  
            2: 'car',          # √î t√¥
            3: 'motorcycle',    # Xe m√°y - quan tr·ªçng nh·∫•t ·ªü VN
            5: 'bus',          # Xe bu√Ωt
            7: 'truck',        # Xe t·∫£i
            9: 'traffic_light' # ƒê√®n giao th√¥ng
        }
        
        # M√†u s·∫Øc d·ªÖ nh√¨n
        self.colors = {
            'person': (0, 255, 0),        # Xanh l√°
            'bicycle': (255, 255, 0),     # V√†ng
            'car': (255, 0, 0),          # ƒê·ªè
            'motorcycle': (255, 0, 255),  # T√≠m - n·ªïi b·∫≠t cho xe m√°y
            'bus': (0, 255, 255),        # Cyan
            'truck': (128, 0, 128),      # T√≠m ƒë·∫≠m
            'traffic_light': (0, 165, 255) # Cam
        }
        
        # Threading cho x·ª≠ l√Ω frame
        self.frame_queue = Queue(maxsize=2)  # Gi·ªõi h·∫°n queue size
        self.result_queue = Queue(maxsize=2)
        self.processing = False
        
        print("‚úÖ Kh·ªüi t·∫°o ho√†n th√†nh!")
    
    def _load_model(self):
        """Load model v·ªõi optimization cho RPi"""
        try:
            model_path = self.config['model']['yolo_model']
            
            # Th·ª≠ load model YOLO11n tr∆∞·ªõc
            if "yolov8" in model_path:
                print("‚ö†Ô∏è ƒêang d√πng YOLOv8, khuy√™n d√πng YOLO11n cho RPi")
                model_path = "yolo11n.pt"  # T·ª± ƒë·ªông chuy·ªÉn sang YOLO11n
            
            self.model = YOLO(model_path)
            
            # Export sang NCNN n·∫øu ch∆∞a c√≥
            if self.use_ncnn:
                ncnn_path = model_path.replace('.pt', '_ncnn_model')
                try:
                    # Th·ª≠ load NCNN model
                    self.model = YOLO(ncnn_path)
                    print("‚úÖ ƒê√£ load NCNN model (t·ªëi ∆∞u cho RPi)")
                except:
                    # Export sang NCNN n·∫øu ch∆∞a c√≥
                    print("üîÑ ƒêang export model sang NCNN format...")
                    temp_model = YOLO(model_path)
                    temp_model.export(format="ncnn", imgsz=self.input_size[0])
                    self.model = YOLO(ncnn_path)
                    print("‚úÖ Export NCNN ho√†n th√†nh!")
            else:
                print("üìù S·ª≠ d·ª•ng PyTorch model (ch·∫≠m h∆°n tr√™n RPi)")
                
        except Exception as e:
            print(f"‚ùå L·ªói load model: {e}")
            # Fallback v·ªÅ YOLOv8n
            self.model = YOLO("yolov8n.pt")
    
    def detect_optimized(self, frame: np.ndarray) -> List[Dict]:
        """
        Detect objects v·ªõi optimization cho real-time
        
        Args:
            frame: Input frame
            
        Returns:
            List detections v·ªõi bbox, confidence, class
        """
        # Resize frame ƒë·ªÉ tƒÉng t·ªëc
        height, width = frame.shape[:2]
        resized_frame = cv2.resize(frame, self.input_size)
        
        # Inference
        results = self.model(
            resized_frame,
            conf=self.confidence,
            iou=self.iou_threshold,
            verbose=False,
            device='cpu'  # Ch·ªâ ƒë·ªãnh d√πng CPU cho RPi
        )
        
        detections = []
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # Scale bounding box v·ªÅ k√≠ch th∆∞·ªõc g·ªëc
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    x1 = int(x1 * width / self.input_size[0])
                    y1 = int(y1 * height / self.input_size[1])
                    x2 = int(x2 * width / self.input_size[0])
                    y2 = int(y2 * height / self.input_size[1])
                    
                    confidence = float(box.conf[0].cpu().numpy())
                    class_id = int(box.cls[0].cpu().numpy())
                    
                    # Ch·ªâ l·∫•y objects quan tr·ªçng cho giao th√¥ng
                    if class_id in self.traffic_classes:
                        detections.append({
                            'bbox': [x1, y1, x2, y2],
                            'confidence': confidence,
                            'class_id': class_id,
                            'class_name': self.traffic_classes[class_id]
                        })
        
        return detections
    
    def draw_detections_simple(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """
        V·∫Ω detections ƒë∆°n gi·∫£n, t·ªëi ∆∞u cho RPi
        """
        annotated_frame = frame.copy()
        
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            confidence = detection['confidence']
            class_name = detection['class_name']
            
            color = self.colors.get(class_name, (255, 255, 255))
            
            # V·∫Ω bounding box m·ªèng h∆°n ƒë·ªÉ ti·∫øt ki·ªám computation
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 1)
            
            # Label ƒë∆°n gi·∫£n
            label = f"{class_name}: {confidence:.1f}"  # Ch·ªâ 1 ch·ªØ s·ªë th·∫≠p ph√¢n
            cv2.putText(annotated_frame, label, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        return annotated_frame
    
    def get_traffic_stats_fast(self, detections: List[Dict]) -> Dict:
        """Th·ªëng k√™ nhanh cho real-time display"""
        stats = {
            'total': len(detections),
            'vehicles': sum(1 for d in detections if d['class_name'] in ['car', 'motorcycle', 'bus', 'truck']),
            'people': sum(1 for d in detections if d['class_name'] == 'person'),
            'motorcycles': sum(1 for d in detections if d['class_name'] == 'motorcycle'),
        }
        return stats
    
    def process_frame_threaded(self, frame: np.ndarray):
        """Process frame trong thread ri√™ng ƒë·ªÉ kh√¥ng block main loop"""
        if not self.frame_queue.full():
            self.frame_queue.put(frame.copy())
    
    def _processing_thread(self):
        """Thread x·ª≠ l√Ω frame"""
        while self.processing:
            if not self.frame_queue.empty():
                frame = self.frame_queue.get()
                detections = self.detect_optimized(frame)
                
                if not self.result_queue.full():
                    self.result_queue.put(detections)
            time.sleep(0.01)  # Ng·∫Øt ng·ªß ng·∫Øn
    
    def start_threading(self):
        """B·∫Øt ƒë·∫ßu threading mode"""
        self.processing = True
        self.thread = threading.Thread(target=self._processing_thread, daemon=True)
        self.thread.start()
        print("üîÑ Threading mode started")
    
    def stop_threading(self):
        """D·ª´ng threading mode"""
        self.processing = False
        if hasattr(self, 'thread'):
            self.thread.join()
        print("‚èπÔ∏è Threading mode stopped")
    
    def get_latest_results(self) -> Optional[List[Dict]]:
        """L·∫•y k·∫øt qu·∫£ detection m·ªõi nh·∫•t"""
        if not self.result_queue.empty():
            return self.result_queue.get()
        return None

# Utility functions
def optimize_camera_settings(cap: cv2.VideoCapture) -> cv2.VideoCapture:
    """
    T·ªëi ∆∞u settings camera cho RPi
    """
    # Gi·∫£m resolution ƒë·ªÉ tƒÉng FPS
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    # Buffer size = 1 ƒë·ªÉ gi·∫£m latency
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    
    print("üìπ Camera settings optimized for RPi")
    return cap

def measure_fps(func):
    """Decorator ƒëo FPS"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        fps = 1.0 / (end_time - start_time) if (end_time - start_time) > 0 else 0
        print(f"‚ö° Current FPS: {fps:.1f}")
        return result
    return wrapper

# Demo real-time detection
if __name__ == "__main__":
    print("üöÄ Demo Optimized Traffic Detection cho Raspberry Pi")
    
    # Kh·ªüi t·∫°o detector
    detector = OptimizedDetector(use_ncnn=True)
    
    # M·ªü camera
    cap = cv2.VideoCapture(0)
    cap = optimize_camera_settings(cap)
    
    if not cap.isOpened():
        print("‚ùå Kh√¥ng th·ªÉ m·ªü camera")
        exit()
    
    print("üìπ B·∫Øt ƒë·∫ßu real-time detection...")
    print("Nh·∫•n 'q' ƒë·ªÉ tho√°t, 's' ƒë·ªÉ b·∫≠t/t·∫Øt stats")
    
    show_stats = True
    fps_counter = 0
    start_time = time.time()
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Detect objects
            frame_start = time.time()
            detections = detector.detect_optimized(frame)
            detection_time = time.time() - frame_start
            
            # V·∫Ω detections
            annotated_frame = detector.draw_detections_simple(frame, detections)
            
            # Hi·ªÉn th·ªã th·ªëng k√™
            if show_stats:
                stats = detector.get_traffic_stats_fast(detections)
                
                # FPS v√† th√¥ng tin
                cv2.putText(annotated_frame, f"FPS: {1.0/detection_time:.1f}", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(annotated_frame, f"Objects: {stats['total']}", 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(annotated_frame, f"Vehicles: {stats['vehicles']} | People: {stats['people']}", 
                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(annotated_frame, f"Motorcycles: {stats['motorcycles']}", 
                           (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1)
            
            # Hi·ªÉn th·ªã frame
            cv2.imshow('Optimized Traffic Detection - RPi', annotated_frame)
            
            # X·ª≠ l√Ω ph√≠m
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                show_stats = not show_stats
                print(f"Stats display: {'ON' if show_stats else 'OFF'}")
            
            fps_counter += 1
            
            # Hi·ªÉn th·ªã FPS trung b√¨nh m·ªói 30 frames
            if fps_counter % 30 == 0:
                avg_fps = fps_counter / (time.time() - start_time)
                print(f"üìä Average FPS: {avg_fps:.1f}")
    
    except KeyboardInterrupt:
        print("\nüõë D·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        print("‚úÖ Cleanup ho√†n th√†nh")